---
layout: post
title: Quickstart on LLMs
category: [deep-learning, llms]
date: 2023-12-01
---

1. TOC
{:toc}

Updated on May, 25

## Quickstart on LLMs

This simple index is aimed at helping one get started in the domain of LLMs. Obviously, the area is continuously evolving, so this index may become irrelevant soon. The most basic requisite is an understanding of Neural networks.

### Understand the lay of the land

The first goal should be to understand what LLMs are, why they follow certain structures (decoder-only transformer networks), and how they can be used.

1. Research papers
   1. [Attention is all you need](https://arxiv.org/abs/1706.03762)
   2. [BERT](https://arxiv.org/abs/1810.04805)
   3. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
   4. [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)
   5. [Improving language understanding with unsupervised learning](https://openai.com/index/language-unsupervised/)
2. [State of GPT by Andrej Karpathy](https://www.youtube.com/watch?v=bZQun8Y4L2A)
3. [Don't teach. Incentivize](https://www.youtube.com/watch?app=desktop&v=kYWUEV_e2ss&feature=youtu.be)
4. [Stanford CS229 Building Large Language Models](https://www.youtube.com/watch?v=9vM4p9NN0Ts)
5. [The illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
6. [The illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)
7. [How GPT3 works](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
8. [Building LLM applications for production by Chip Huyen](https://huyenchip.com/2023/04/11/llm-engineering.html)
9. [RLHF: Reinforcement Learning from Human Feedback by Chip Huyen](https://huyenchip.com/2023/05/02/rlhf.html)

### Learn how to use LLMs

The second goal should be to understand how to use LLMs. Both locally and cloud-hosted ones. What does it take to run an LLM?

1. [A Hackers' Guide to Language Models by Jeremy Howard](https://www.youtube.com/watch?v=jkrNMKz9pWU)
2. [Optimizing your LLM in production](https://huggingface.co/blog/optimize-llm)
3. [KV Cache brief explainer](https://www.youtube.com/watch?v=80bIUggRJf4)
4. [Understanding Llama2: KV Cache, Grouped Query Attention, Rotary Embedding and More](https://ai.plainenglish.io/understanding-llama2-kv-cache-grouped-query-attention-rotary-embedding-and-more-c17e5f49a6d7)
5. [LoRA explained (and a bit about precision and quantization)](https://www.youtube.com/watch?v=t509sv5MT0w)
6. [Paged Attention](https://blog.vllm.ai/2023/06/20/vllm.html)
7. [How continuous batching enables 23x throughput in LLM inference while reducing p50 latency](https://www.anyscale.com/blog/continuous-batching-llm-inference)
8. [How is Llama cpp possible](https://finbarr.ca/how-is-llama-cpp-possible/)
9. [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/)

From here, it depends on which area you are specifically interested in. Broadly (according to me), there are 3 tracks and requires sufficient time to build mastery.

1. Inference optimization, [training workload optimizations](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
2. Pre-training and post-training - Can be for different modalities
3. RL
