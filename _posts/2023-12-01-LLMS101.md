---
layout: post
title: Quickstart on LLMs
category: [deep-learning, llms]
date: 2023-12-01
---

1. TOC
{:toc}

## Quickstart on LLMs

This simple index is aimed to help one get started in the domain of LLMs. Obviously, the area is continuously evolving so this index may become irrelevant soon.

### Beginner

1. If new to LLMs, start with this video to get an overview: [State of GTP by Andrej Karpathy](https://www.youtube.com/watch?v=bZQun8Y4L2A)
2. [Building LLM applications for production by Chip Huyen](https://huyenchip.com/2023/04/11/llm-engineering.html)
3. [Don't teach. Incentivize](https://www.youtube.com/watch?app=desktop&v=kYWUEV_e2ss&feature=youtu.be)

### Intermediate

1. [A Hackers' Guide to Language Models by Jeremy Howard](https://www.youtube.com/watch?v=jkrNMKz9pWU)
   1. "The best way to learn about language models is to use them" from the video. And this video will teach you to exactly do that
2. Master blog of how to serve LLMs in production: [Optimizing your LLM in production](https://huggingface.co/blog/optimize-llm)
3. How LLMs are trained?
   1. [RLHF: Reinforcement Learning from Human Feedback by Chip Huyen](https://huyenchip.com/2023/05/02/rlhf.html)
4. How to serve LLMs in memory-constrained environments?
   1. [KV Cache brief explainer](https://www.youtube.com/watch?v=80bIUggRJf4)
   2. [Understanding Llama2: KV Cache, Grouped Query Attention, Rotary Embedding and More](https://ai.plainenglish.io/understanding-llama2-kv-cache-grouped-query-attention-rotary-embedding-and-more-c17e5f49a6d7)
5. Understanding basics of LoRa for training LLMs in memory-constrained environments: [LoRA explained (and a bit about precision and quantization)](https://www.youtube.com/watch?v=t509sv5MT0w)
6. Learn about quantization and flash attention

### Advanced

1. [vLLMs](https://blog.vllm.ai/2023/06/20/vllm.html): Serving LLMs faster
